from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import re
import csv
import urllib.parse
import logging

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Chrome options
chrome_options = Options()
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)
# chrome_options.add_argument("--headless")  # ‡πÄ‡∏õ‡∏¥‡∏î‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î browser

driver = webdriver.Chrome(options=chrome_options)
driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
wait = WebDriverWait(driver, 20)

def wait_for_page_load(driver, timeout=10):
    try:
        WebDriverWait(driver, timeout).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        time.sleep(3)
        return True
    except Exception as e:
        logger.error(f"‚ùå Error while waiting for page load: {e}")
        return False

def find_search_box(driver):
    selectors = [
        "input[type='search']",
        "input[name*='search']",
        "input[placeholder*='‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤']",
        "input[class*='search']",
        "input[id*='search']",
        "input"
    ]
    for selector in selectors:
        try:
            element = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            if element.is_displayed() and element.is_enabled():
                return element
        except:
            continue
    return None

def make_absolute_url(url, base_url):
    if url.startswith('http'):
        return url
    return urllib.parse.urljoin(base_url, url)

def extract_course_info(driver):
    soup = BeautifulSoup(driver.page_source, "html.parser")
    course_links = []
    links = soup.find_all("a", href=True)
    for link in links:
        text = link.get_text(strip=True)
        if "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå" in text or "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå" in text:
            course_links.append({
                'url': link['href'],
                'title': text,
                'type': 'link_text'
            })
    return course_links

def extract_university_name(soup):
    university_name = ""

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏•‡∏¥‡∏á‡∏Å‡πå /universities/
    try:
        links = soup.find_all('a', href=re.compile(r'/universities/\d+'))
        for link in links:
            text = link.get_text(strip=True)
            if any(k in text for k in ['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô', 'University', 'College']):
                university_name = text
                break
    except:
        pass

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: selectors
    if not university_name:
        try:
            selectors = [
                'h1', 'h2', 'h3', '.university', '.school', '.institution',
                '[class*="university"]', '[class*="school"]', '[id*="university"]',
                '.college', '[class*="college"]', '.univ', '[class*="univ"]'
            ]
            for selector in selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text(strip=True)
                    if any(k in text for k in ['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô', 'University', 'College']):
                        if not any(c in text for c in ['‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°', 'Engineering', '‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']):
                            university_name = text
                            break
                if university_name:
                    break
        except:
            pass

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: breadcrumbs
    if not university_name:
        try:
            breadcrumbs = soup.find_all(['nav', 'ol', 'ul'], class_=re.compile(r'breadcrumb|nav', re.I))
            for bc in breadcrumbs:
                links = bc.find_all('a')
                for link in links:
                    text = link.get_text(strip=True)
                    if any(k in text for k in ['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô']):
                        university_name = text
                        break
                if university_name:
                    break
        except:
            pass

    # ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 4: regex
    if not university_name:
        try:
            text = soup.get_text()
            patterns = [
                r'(‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢[^\n\r:]{1,60})',
                r'(‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢[^\n\r:]{1,60})',
                r'(‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô[^\n\r:]{1,60})',
                r'([A-Z][a-z]+ University)',
                r'([A-Z][a-z]+ College)',
            ]
            for pattern in patterns:
                matches = re.findall(pattern, text)
                if matches:
                    university_name = min(matches, key=len).strip()
                    break
        except:
            pass

    return university_name

def extract_detail_from_course(url):
    driver.get(url)
    wait_for_page_load(driver)
    soup = BeautifulSoup(driver.page_source, "html.parser")

    university = extract_university_name(soup)

    # ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£
    course_name = ""
    try:
        h1 = soup.find('h1')
        if h1:
            course_name = h1.get_text(strip=True)
    except:
        pass

    # ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
    course_name_eng = ""
    try:
        eng_tag = soup.find(text=re.compile(r'‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©|‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©'))
        if eng_tag and eng_tag.find_parent():
            course_name_eng = eng_tag.find_parent().find_next_sibling().get_text(strip=True)
    except:
        pass

    # ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢
    fee = ""
    try:
        dt_tags = soup.find_all('dt')
        for dt in dt_tags:
            if any(k in dt.get_text() for k in ['‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢', '‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô', '‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏ô']):
                dd = dt.find_next_sibling('dd')
                if dd:
                    fee = dd.get_text(strip=True)
                    break
    except:
        pass

    return {
        'url': url,
        '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': university,
        '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢': fee,
        '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': course_name,
        '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©': course_name_eng
    }

def search_and_extract(driver, keyword):
    logger.info(f"\nüîç Searching: {keyword}")
    driver.get("https://course.mytcas.com/")
    wait_for_page_load(driver)

    search_box = find_search_box(driver)
    if not search_box:
        logger.warning("‚ùå Search box not found")
        return []

    search_box.clear()
    search_box.send_keys(keyword)
    search_box.send_keys(Keys.RETURN)

    wait_for_page_load(driver)
    time.sleep(3)
    return extract_course_info(driver)

def main():
    keywords = ["‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå"]
    all_course_data = []

    for keyword in keywords:
        course_links = search_and_extract(driver, keyword)
        for course in course_links:
            absolute_url = make_absolute_url(course['url'], "https://course.mytcas.com/")
            try:
                detailed_info = extract_detail_from_course(absolute_url)
                detailed_info['url'] = absolute_url
                all_course_data.append(detailed_info)
                logger.info(f"‚úÖ Extracted course: {detailed_info['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£']}")

                # üëâ ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô" (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î)
                if detailed_info['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢'].strip().startswith("‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô"):
                    logger.info(f"üõë ‡∏û‡∏ö‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô': {detailed_info['‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢']}")

            except Exception as e:
                logger.warning(f"‚ùå Failed to extract from {absolute_url}: {e}")
            time.sleep(2)

    # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á CSV
    if all_course_data:
        with open("perfect.csv", "w", newline='', encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=['url', '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢', '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢', '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©'])
            writer.writeheader()
            writer.writerows(all_course_data)
        logger.info("üíæ Data saved to perfect.csv")
    else:
        logger.warning("‚ö†Ô∏è No course data found")

    driver.quit()


if __name__ == "__main__":
    main()
